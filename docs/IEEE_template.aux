\relax 
\citation{bishop2006pattern}
\citation{hastie2009elements}
\citation{cover1967nearest}
\citation{vapnik1999overview}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}\protected@file@percent }
\newlabel{sec:introduction}{{I}{1}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {I-A}}Motivation: The Small-Sample Challenge}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {I-B}}Limitations of Existing Approaches}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {I-C}}Proposed Solution: SASKC}{1}{}\protected@file@percent }
\citation{scikit-learn}
\citation{cover1967nearest}
\citation{vapnik1999overview}
\citation{hastie2009elements}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {I-D}}Contributions}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{}\protected@file@percent }
\newlabel{sec:related_work}{{II}{2}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Instance-Based Learning and KNN Variants}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Kernel Methods and SVMs}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}Feature Weighting and Metric Learning}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Methodology: The SASKC Algorithm}{2}{}\protected@file@percent }
\newlabel{sec:methodology}{{III}{2}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Phase 1: Statistical Profiling and Weight Generation}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-A}1}Theoretical Foundation: The Mahalanobis Connection}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-A}2}Variance-Based Weighting}{2}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Conceptual Illustration of Feature Variance Weighting. The algorithm automatically identifies high-variance "noise" dimensions (red) and assigns them low weights, while low-variance "signal" dimensions (green) are preserved.}}{3}{}\protected@file@percent }
\newlabel{fig:variance}{{1}{3}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Phase 2: The Statistical Kernel}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Phase 3: Rank-Based Voting Mechanism}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Rank-Based Voting Mechanism. The decision boundary is refined by weighting neighbors not just by their spatial proximity, but also by their ordinal rank. This prevents distant outliers from outvoting the closest, most relevant samples.}}{3}{}\protected@file@percent }
\newlabel{fig:voting}{{2}{3}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}Algorithm and Complexity Analysis}{3}{}\protected@file@percent }
\citation{uci}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces SASKC Training and Prediction}}{4}{}\protected@file@percent }
\newlabel{alg:saskc}{{1}{4}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-D}1}Time Complexity}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-D}2}Space Complexity}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}System Architecture}{4}{}\protected@file@percent }
\newlabel{sec:architecture}{{IV}{4}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Stage 1: Data Ingestion and Profiling}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Stage 2: Preprocessing and Standardization}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-C}}Stage 3: The SASKC Core Engine}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-D}}Stage 4: Inference and Voting}{4}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces SASKC System Architecture. The pipeline ensures that statistical properties learned during training are consistently applied during inference, maintaining the integrity of the adaptive kernel.}}{4}{}\protected@file@percent }
\newlabel{fig:architecture}{{3}{4}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experimental Setup}{4}{}\protected@file@percent }
\newlabel{sec:experiments}{{V}{4}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-A}}Dataset Characterization: UCI Wine}{4}{}\protected@file@percent }
\citation{scikit-learn}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces PCA Projection of the Wine Dataset. The first two principal components explain only a fraction of the variance, indicating that the discriminative information is spread across higher dimensionsâ€”precisely where SASKC's feature weighting excels.}}{5}{}\protected@file@percent }
\newlabel{fig:pca}{{4}{5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-B}}Baseline Classifiers and Hyperparameters}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-C}}Noise Injection Protocol}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-D}}Evaluation Methodology}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}Results and Discussion}{5}{}\protected@file@percent }
\newlabel{sec:results}{{VI}{5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-A}}Quantitative Performance Analysis}{5}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Model Performance (Mean $\pm $ Std Dev over 100 Runs)}}{5}{}\protected@file@percent }
\newlabel{tab:results}{{I}{5}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-A}1}Comparison with Instance-Based Methods}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-A}2}Comparison with Tree-Based Methods}{5}{}\protected@file@percent }
\bibcite{bishop2006pattern}{1}
\bibcite{hastie2009elements}{2}
\bibcite{cover1967nearest}{3}
\bibcite{vapnik1999overview}{4}
\bibcite{scikit-learn}{5}
\bibcite{uci}{6}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-B}}Stability Analysis}{6}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces F1-Score Stability Analysis. SASKC shows consistent performance across 100 random splits, highlighting its reliability. Note the large variance in Decision Tree performance, which SASKC successfully avoids.}}{6}{}\protected@file@percent }
\newlabel{fig:stability}{{5}{6}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-C}}Noise Robustness and Kernel Behavior}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VII}Conclusion and Future Work}{6}{}\protected@file@percent }
\newlabel{sec:conclusion}{{VII}{6}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Noise Robustness. SASKC maintains high accuracy even when features are corrupted, demonstrating the efficacy of the statistical kernel. Standard distance metrics degrade rapidly under noise.}}{6}{}\protected@file@percent }
\newlabel{fig:noise}{{6}{6}{}{}{}}
\@writefile{toc}{\contentsline {section}{References}{6}{}\protected@file@percent }
\gdef \@abspage@last{6}
